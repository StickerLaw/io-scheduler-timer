\documentclass[12pt, a4paper]{article} %minska pointen om du vill ha mindre bokstäver

\usepackage[english]{babel}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc} 	

\usepackage{amsmath}	% Om du vill använda AMSLaTeX 
\usepackage{amssymb}	% Om du vill använda AMS symboler
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage[hidelinks]{hyperref}
\usepackage{csvsimple}
\usepackage{booktabs}
\usepackage{siunitx} % Formats the units and values

\usepackage[parfill]{parskip}

\usepackage{subcaption} % för flera figurer i en

% Färger för exempelkoden
\usepackage{xcolor}
\definecolor{backcol}{gray}{0.95} % bakgrundfärg
\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24} % bl.a. kommentarer i koden

\usepackage{listings} % för exempelkod
% Inställningar
\lstset{
	basicstyle=\ttfamily\scriptsize, % monospace, mindre bokstäver
	basewidth  = {.5em,0.5em}, % minskar avståndet mellan bokstäverna (passar fonten)
	numbers=left, numberstyle=\tiny, 
	frame=tb, % streck top och bottom
	breaklines=true, % radbyte vid behov
	backgroundcolor = \color{backcol},
	keywordstyle=\color{blue}, 
	commentstyle=\color{darkpastelgreen},
	captionpos=t,
	framexleftmargin=2mm, % padding
	xleftmargin=2mm, % lägg till marginal för att hållas i linje med texten
	framexrightmargin=2mm,
	xrightmargin=2mm,
	}

\sisetup{exponent-product = \cdot, output-product = \cdot}

\newtheorem{remark}{Remark}

\captionsetup{width=0.8\textwidth}

\title{Operating systems -- Assignment 3\\I/O Scheduling}

\author{Lennart Jern\\
	CS: ens16ljn\\ \\ \textbf{Teacher}\\ Ahmed Aley}


\begin{document}


\maketitle

\newpage


\section{Introduction}

Disk access is significantly slower than any CPU operation and is often a bottle neck when it comes to performance.
In this report I compare the performance of three I/O schedulers: ``cfq'', ``noop'' and ``deadline''.
Additionally, all tests are done on two different types of disks: a USB flash drive and a hard disk drive (also connected via USB).

The benchmark used consists of a search for files in a hierarchy of directories.
In other words, this report is focused on seek times since nothing is written to disk and just meta data (such as file name and directory content) is read.

\section{Method}

The benchmark consists of a program (\texttt{mfind}, see listing \ref{mfind}) that searches for files in a hierarchy of directories and prints out the time taken for each call to \texttt{readdir}.
A script (\texttt{timer.sh}, see listing \ref{timer}) is used to run \texttt{mfind} in four parallel processes, ten times for each of the three schedulers, and collects all timing data in log files.
The partition is unmounted between each run to make sure that no files are cached.

Processing of the data is done by \texttt{stats.py} (see listing \ref{stats}) in order to obtain some statistical properties.
The python library Pandas\footnote{\url{http://pandas.pydata.org/}} proved very helpful in this regard.

\begin{remark}
	The program \texttt{mfind} was originally made for another assignment and was here simplified and modified to measure the time required for each I/O request.
	In addition to \texttt{mfind.c} the program also consists of a parser (\texttt{parser.c} and \texttt{parser.h} in listings \ref{parser} and \ref{parserh}) and the linked list that keeps track of what directories to search (\texttt{list.c} and \texttt{list.h} in listings \ref{list} and \ref{listh}).
	The complete program can be compiled using the make file in listing \ref{make}.
\end{remark}

A script was also used to create the tree of searched directories in order to make the benchmark easier to reproduce.
This script can be found in listing \ref{test-files}.

All tests were run on my personal computer with the specifications seen in table \ref{spec}.
The drives used was a Kingston DataTraveler 1 GB and a Verbatim 500 GB portable 2.5'' HDD.
Both drives were connected to a USB 2.0 port.

\begin{table}[ht]
	\centering
	\begin{tabular}{ll}
		\toprule
		Component & Specification \\
		\midrule
		OS: & Fedora 25 \\
		Kernel: & Linux 4.8.13-300.fc25.x86\_64 \\
		CPU: & Intel Core i5-2500K CPU @ 3.7GHz \\
		RAM: & 7965MiB \\
		GCC: & 6.2.1 \\
		Bash: & 4.3.43 \\
		Python: & 3.5.2 \\
		Pandas: & 0.18.1 \\
		Matplotlib: & 1.5.3 \\
		\hline
	\end{tabular}
	\caption{Test system specification.}
	\label{spec}
\end{table}

\section{Results}

The calculated statistical properties of the timing data can be seen in tables \ref{kingston} and \ref{verbatim}.
Only slight differences can be seen between the schedulers for the flash drive.
Most noticeably, the maximum time required is considerably greater for cfq than for the other schedulers.
This also reflects on a greater standard deviation.

\begin{table}[ht]
	\centering
	\csvreader[tabular=lccc,
	head to column names,
	table head=\toprule & cfq & deadline & noop \\ \midrule,
	table foot=\hline
	]{stats-kingston.csv}{}
	{\csvcoli & \num{\csvcolii} & \num{\csvcoliii} & \num{\csvcoliv}}
	\caption{Statistical properties for the Kingston flash drive. The time values are measured in seconds.}
	\label{kingston}
\end{table}

Looking at the means and medians (50 \% quantile), we see that noop has the lowest mean, but is beaten by deadline when it comes to the median.
Noop also has the lowest minimum time required.

The picture is quite different when looking at table \ref{verbatim}.
This is for a traditional spinning hard drive, so the cfq scheduler get to shine.
The cfq scheduler has the best mean, minimum, median and 75 \% quantile.
All this comes at a cost, of course, the maximum time is far greater than for the other schedulers.
Despite this, however, the cfq still manages to have the lowest standard deviation among the three.

\begin{table}[ht]
	\centering
	\csvreader[tabular=lccc,
		head to column names,
		table head=\toprule & cfq & deadline & noop \\ \midrule,
		table foot=\hline
	]{stats-verbatim.csv}{}
	{\csvcoli & \num{\csvcolii} & \num{\csvcoliii} & \num{\csvcoliv}}
	\caption{Statistical properties for the Verbatim HDD. The time values are measured in seconds.}
	\label{verbatim}
\end{table}

A comparison of the two drives reveals that the scheduler should be chosen depending on the type of drive for best performance.
On a traditional spinning drive, there is more to win by ordering the requests in an efficient way.
However, the ordering does not matter much on a solid state drive, since there is no physical movement required.
In that case is is better not to spend time finding the ``perfect order'' and thus delay some requests.

In order to improve the noop scheduler on traditional HDDs we would have to allow a higher maximum time.
This would give the scheduler a better possibility to control the ordering of the requests.

Similarly, the cfq scheduler would probably be faster on flash drives if it just sent all requests in the order they came in, instead of reordering them.
The deadline scheduler is similar in performance to the noop scheduler on both drives and could probably improve it's performance on HDDs in the same way.
My guess is that the deadline and noop schedulers would show greater differences if the requests were longer.



\section{Final thoughts and lessons learned}

Caching of data gave some problems at first, but this could easily be solve by unmounting the drive between each run.
However, the unmounting was quite problematic for the HDD since it would sometimes remain busy even after a \texttt{sync} call.
To solve this, a one second sleep was added to the script.

The problem with \texttt{sync} and \texttt{umount} made me quite suspicious of the data gathered.
I am uncertain about what really should be timed and how to make sure that the I/O requests are actually done on the disk and not just in some cache.
Perhaps more fine tuned control would be needed to solve these problems, with calls to \texttt{syncfs} periodically.
But that makes it hard to determine the actual time taken for individual requests.

One thing is clear: the I/O scheduler can definitely have a great impact on performance and it can be quite tricky to accurately measure the performance of these schedulers.
Some theoretical background would have been interesting and helpful if time would have allowed for that during the lectures.

\clearpage
\appendix

\section{Code listings}

\lstinputlisting[language=c, caption=mfind.c, label=mfind]{../code/mfind.c}

\lstinputlisting[language=bash, caption=timer.sh, label=timer]{../code/timer.sh}

\lstinputlisting[language=python, caption=stats.py, label=stats]{../code/stats.py}

\lstinputlisting[language=c, caption=parser.c, label=parser]{../code/parser.c}

\lstinputlisting[language=c, caption=parser.h, label=parserh]{../code/parser.h}

\lstinputlisting[language=c, caption=list.c, label=list]{../code/list.c}

\lstinputlisting[language=c, caption=list.h, label=listh]{../code/list.h}

\lstinputlisting[caption=Makefile, label=make]{../code/Makefile}

\lstinputlisting[language=bash, caption=generate\_test\_files.sh, label=test-files]{../code/generate_test_files.sh}



\end{document}
